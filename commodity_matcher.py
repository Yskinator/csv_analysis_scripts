import argparse
import time

import os
import sys
import csv
import concurrent.futures
import regex as re
import pandas
# if "LOCAL" in os.environ:
import file_utils
import top_category_matcher
import brand_extract_parallel
# else:
#     from . import file_utils
#     from . import top_category_matcher

from matcher import most_matching_words, best_n_results, to_base_word_set

csv.field_size_limit(int(sys.maxsize/100000000000))

FIELDNAMES = ["Stock & Site", "Site", "Stock Code", "text", "OEM Field", "Commodity", "Commodity Code", "Jaccard", "Match Number"]

def match_commodities(stock_with_top_categories, jaccard_threshold, topn, parallel=True):
    """Match commodities to stocks.
    Requires csv:s generated by generate_top_category_files to be in top_category_files/.

    Arguments:
    stock_with_top_categories -- A list of dictionaries, each with keys "Description", "id", "Top Categories", and "Brands".
    jaccard_threshold (float) -- if jaccard_index is lower than threshold, re-run with all top categories.
    parallel (boolean) -- Whether to use concurrency or not. Defaults to True.

    Returns:
    List of dictionaries with the same keys as stock_with_top_categories, and the keys "Commodity", "Commodity Code", and "Jaccard".
    """
    brands = get_brands()
    abbrevs = file_utils.read_csv("desc_abbrevs.csv")
    #Fetches all the allowed top categories.
    tcs = top_category_matcher.non_excluded_top_categories()
    commodities = {tc: get_commodities_for_top_category(tc, abbrevs) for tc in tcs}
    if parallel:
        with concurrent.futures.ProcessPoolExecutor() as executor:
            futures = []
            for row in stock_with_top_categories:
                futures.append(executor.submit(match_commodities_for_row, row, jaccard_threshold, commodities, brands, topn, abbrevs))
            updated_rows = [future.result() for future in futures]
    else:
        updated_rows = [match_commodities_for_row(row, jaccard_threshold, commodities, brands, topn, abbrevs) for row in stock_with_top_categories]
    return updated_rows

def get_commodities_for_top_category(top_category, abbrevs=[]):
    return get_commodities_for_top_categories([top_category], abbrevs)

def get_commodities_for_top_categories(top_categories, abbrevs=[]):
    """Given a list of top categories:
        (1) go through the matching files and
        (2) compose a list of all commodities contained in those files.

        Arguments:
        top_categories -- list of names of top categories to fetch

        Returns:
        List of commodities in the given top categories."""
    commodities = {}
    for top_cat in top_categories:
        rows = file_utils.read_csv("top_category_files/" + top_cat + ".csv")
        for row in rows:
            if row["Commodity Name"] in commodities:
                print("Duplicate commodity: " + row["Commodity Name"])
            commodities[row["Commodity Name"]] = {"Commodity Code": row["Commodity"], "Preprocessed": to_base_word_set(row["Commodity Name"], abbrevs)}
    return commodities

def match_commodities_for_row(row, jaccard_threshold, commodities_by_tc, brands=[], topn=1, abbrevs=[]):
    """Take a row dictionary and return best-matching commodities.

    Arguments:
    row -- A dictionary with the fields ''
    jaccard_threshold -- if the Jaccard index of the best match is below threshold, rerun with more categories
    commodities_by_tc -- A dictionary mapping top category names to lists of commodities
    brands -- A list of brand names to ignore
    topn -- Amount of matches to return for each row

    Output:
    The input row with additional fields 'Commodity' 'Commodity_Code' and 'Jaccard' for each match.
    """
    desc = to_base_word_set(row["Description"], abbrevs)
    brands = set(brands)
    print("Row " + row["id"] + ", matching commodities.")
    tc_string = row["Top Categories"].replace('"', "")
    tcs = filter(None, tc_string.split(";"))
    commodities = {}
    for tc in tcs:
        commodities.update(commodities_by_tc[tc])
    results, scores = most_matching_words(desc, sentences_preprocessed=commodities, number_of_results=topn, words_to_exclude=brands)

    #RE-RUN MATCHING IF LOW JACCARD SCORES
    if scores[0] < jaccard_threshold:
        #Get ALL top_category files minus the ones we checked before
        print("Low Jaccard score, checking other categories.")
        tcs = set(commodities_by_tc.keys()) - set(tcs)
        commodities = {}
        for tc in tcs:
            commodities.update(commodities_by_tc[tc])
        more_results, more_scores = most_matching_words(desc, sentences_preprocessed=commodities, number_of_results=topn, words_to_exclude=brands)
        #{**x, **y} merges two dictionaries
        jaccard_scores_dict_all_results = {**dict(zip(results, scores)), **dict(zip(more_results, more_scores))}
        results, scores = best_n_results(jaccard_scores_dict_all_results, n=topn)

    for i, res in enumerate(results):
        postfix = f" {i+1}" if i > 0 else ""
        score = round(scores[i], 2)
        code = commodities[res]["Commodity Code"]
        # Ex. if i == 1, keys == ("Commodity 2", "Commodity Code 2", "Jaccard 2")
        keys = (key+postfix for key in ["Commodity", "Commodity Code", "Jaccard"])
        new_results = dict(zip(keys, (res, code, score)))
        row.update(new_results)
    print("Row " + row["id"] + ", commodities found.")
    return row

def get_brands():
    brands = []
    try:
        rows = file_utils.read_csv('brand_counts.csv')
        for row in rows:
            if row["Brand"] != "":
                brands.append(row["Brand"].lower())
    except FileNotFoundError:
        pass
    return brands

def generate_top_category_files(column_name):
    file_utils.mkdir("top_category_files")
    rows = file_utils.read_csv('unspsc_codes_v3.csv')
    tcs = {}
    for row in rows:
        if row[column_name] not in tcs:
            tcs[row[column_name]] = []
        tcs[row[column_name]].append(row)
    for tc in tcs:
        filename = "top_category_files/" + tc + ".csv"
        print("Saving " + filename)
        file_utils.save_csv(filename, tcs[tc])

def top_category_to_string(top_category_name):
    segment_names = []
    family_names = []
    class_names = []
    commodity_names = []
    rows = file_utils.read_csv("top_category_files/" + top_category_name + ".csv")
    for row in rows:
        seg = row["Segment Name"]
        fam = row["Family Name"]
        cl = row["Class Name"]
        com = row["Commodity Name"]
        if not seg in segment_names:
            segment_names.append(seg)
        if not fam in family_names:
            family_names.append(fam)
        if not cl in class_names:
            class_names.append(cl)
        if not com in commodity_names:
            commodity_names.append(com)
    tc_str = str(segment_names) + " " + str(family_names) + " " + str(class_names) + " " + str(commodity_names)
    tc_str = tc_str.replace("[", "").replace("]", "").replace("'", "").replace(",", "").lower()
    return tc_str

def generate_top_category_string_csv():
    tcs = file_utils.top_category_names()
    rows = []
    for s in tcs:
        print(s)
        row = {"Top Category Name": s, "Top Category String": top_category_to_string(s)}
        rows.append(row)
    file_utils.save_csv("top_category_strings.csv", rows)

def generate_preprocessed_stocks(stock_master):
    """Preprocess stocks from stock_master, combining rows with identical descriptions reversibly.
    If rows with ids 1 and 2 have the same description, combines them into one row with id "1;2".

    Arguments:
    stock_master -- A list of dictionaries, each with keys "text", "id", "Brand".

    Returns:
    A list of dictionaries, each with keys "Description", "id", and "Brands".
    """
    ids = {}
    brands = {}
    forbidden_characters = ["0", "1", "2", "3", "4", "5", "6", "7", "8", "9"]
    for i, row in enumerate(stock_master):
        d_orig = row["text"]
        d_splits = re.findall(r"[\w']+", d_orig)
        d = ""
        for s in d_splits:
            if not any(c in s for c in forbidden_characters):
                d += s + " "
        if d == "":
            d = d_orig
        ids[d] =  [row["id"]] if not d in ids else ids[d] + [row["id"]]
        try:
            brands[d] = [row["Brand"]] if not d in brands else brands[d] + [row["Brand"]]
        except:
            pass
    rows = []
    for d in ids:
        row_numbers = ""
        bs = ""
        for rn in ids[d]:
            row_numbers += str(rn) + ";"
        if d in brands:
            for b in brands[d]:
                bs += str(b) + ";"
        row = {"Description": d, "id": row_numbers, "Brands": bs}
        rows.append(row)
    return rows

def count_field(stock_master, field):
    """Given a list of dictionaries representing rows in a csv, create a list of dictionaries representing counts of each value in column 'field'.

    Arguments:
    stock_master -- list of dictionaries representing rows
    field -- name of column or field to count values of

    Returns:
    A list of dictionaries, each with keys field and "Count" representing a count of the values
    """
    counts = {}
    for row in stock_master:
        try:
            f = row[field]
            if not f in counts:
                counts[f] = 1
            else:
                counts[f] += 1
        except:
            continue
    rows = []
    for f in counts:
        row = {field: f, "Count": counts[f]}
        rows.append(row)
    return rows

def map_preprocessed_to_original(combined_stocks, stocks_with_commodities):
    """Map the commodities from the compressed representation to the original.
    E.g. commodity from row with id "1;2;3" in stocks_with_commodities will be added to rows 1, 2, and 3 in combined_stocks.
    Both combined_stocks and stocks_with_commodities should be lists of dictionaries with an integer "id" field.

    Arguments:
    combined_stocks -- List of dictionaries representing the original data prior to compression
    stocks_with_commodities -- List of dictionaries, compressed data with matched commodities

    Returns:
    A tuple with the rows (list of dictionaries) and fieldnames (list of strings)
    """
    stocks = {}
    print("Generating dictionary from the original data..")
    for row in combined_stocks:
        print("Row id: " + row["id"])
        # Use copy here to avoid modifying the input
        stocks[int(row["id"])] = row.copy()
    for row in stocks_with_commodities:
        ids = filter(None, row["id"].split(";"))
        for i in ids:
            print("Updating row id: " + i)
            for key in row.keys():
                if "Commodity" in key or "Jaccard" in key:
                    stocks[int(i)].update({key: row[key]})
    rows = []
    ids = list(stocks.keys())
    ids.sort()
    for i in ids:
        rows.append(stocks[i])
    return rows

def unpivot_stocks(stocks):
    """Perform an unpivot operation on list of dictionaries representing rows.
    Ex. a row {... "Commodity": "something" ... "Commodity 2": "other"} is converted to two rows:
    [{... "Commodity": "something", "Match Number": 1 ...}, {... "Commodity": "other", "Match Number": 2 ... }]

    Arguments:
    stocks - A list of dictionaries representing rows with keys like "Commodity", "Commodity Code" and "Jaccard"

    Returns:
    A list of dictionaries representing rows with each commodity on its own row
    """
    new_stocks = []
    for stock in stocks:
        base_items = {key: val for (key, val) in stock.items() if "Commodity" not in key and "Jaccard" not in key}
        new_stock = {**base_items, "Commodity": stock["Commodity"], "Commodity Code": stock["Commodity Code"], "Jaccard": stock["Jaccard"], "Match Number": "1"}
        new_stocks.append(new_stock)

        i = 2
        while True:
            if "Commodity "+str(i) in stock:
                new_stock = {**base_items, "Commodity": stock["Commodity "+str(i)], "Commodity Code": stock["Commodity Code "+str(i)], "Jaccard": stock["Jaccard "+str(i)], "Match Number": str(i)}
                new_stocks.append(new_stock)
            else:
                break
            i = i+1
    return new_stocks

def remove_temp_files():
    tmp_files = ["brand_counts.csv", "brands_to_top_categories.csv", "preprocessed_stocks_with_brands.csv", "top_category_strings.csv", "stock_with_commodities.csv", "stock_with_top_categories.csv"]
    for f in tmp_files:
        os.remove(f)

def add_commodities_to_stocks(stock_master, level="Family Name", tc_to_check_count=25, jaccard_threshold=0.3, topn=1, parallel=True, skip_preprocessing=False):
    """stock_master is a list of dicts that must contain keys id, text and Brand. Brand may be an empty string."""
    generate_constant_csvs(level)
    preprocessed = generate_preprocessed_stocks(stock_master)
    if skip_preprocessing:
        preprocessed = stock_master
    brand_counts = count_field(stock_master, "Brand")
    top_category_strings = file_utils.read_csv("top_category_strings.csv")
    stock_with_top_categories = top_category_matcher.match_preprocessed_to_top_categories(preprocessed, top_category_strings, brand_counts, tc_to_check_count = tc_to_check_count)
    print("Matching commodities")
    stock_with_commodities = match_commodities(stock_with_top_categories, jaccard_threshold=jaccard_threshold, topn=topn, parallel=parallel)
    rows = map_preprocessed_to_original(stock_master, stock_with_commodities)
    if skip_preprocessing:
        rows = stock_with_commodities
    rows = unpivot_stocks(rows)
    return rows

def add_commodities_to_dataframe(df):
    df = brand_extract_parallel.detect_brands(df)
    input_rows = df.to_dict("records")
    output_rows, fieldnames = add_commodities_to_stocks(input_rows)
    df_out = pandas.DataFrame(output_rows)
    return df_out

def generate_brand_counts_csv():
    try:
        stock_master = file_utils.read_csv("combined_stock_master_withbrands.csv")
        brands = count_field(stock_master, "Brand")
        file_utils.save_csv("brand_counts.csv", brands, fieldnames=["Brand", "Count"])
    except FileNotFoundError:
        print("Warning: files brand_counts.csv and/or combined_stock_master_withbrands.csv were not found. Brand data will not be used.")

def generate_constant_csvs(level="Family Name"):
    if not file_utils.folder_exists("top_category_files/"):
        generate_top_category_files(level)
    else:
        print("top_category_files/ directory already exists")
    if not file_utils.file_exists("top_category_strings.csv"):
        generate_top_category_string_csv()
    else:
        print("top_category_strings.csv file already exists")
    if not file_utils.file_exists("brand_counts.csv"):
        generate_brand_counts_csv()
    else:
        print("brand_counts.csv file already exists")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Script to allocate items to a UNSPSC product")
    parser.add_argument("filename", help="Filename of the csv file to process.")
    parser.add_argument("-l", "--level", help="Defines the level of top categories to check for matches. Only n categories with the highest probability of containing matches will get checked. Accepts Segment, Family or Class. Default is Family.", choices=["Segment", "Family", "Class"], default="Family")
    parser.add_argument("-n", "--num_to_check", help="Number of top categories to check for each row. Higher values mean slower but more accurate matching. Default value 25.", type=int, default=25)
    parser.add_argument("-o", "--output", help="Save output to file with the given filename. If argument is not present, the output is instead printed to console in an abbreviated form.")
    parser.add_argument("-j", "--jaccard", help="Sets the Jaccard threshold. If the Jaccard score of the best match is below the threshold, reruns the search for all top categories to find the best possible match. Default value is 0.3.", type=float, default=0.3)
    parser.add_argument("-m", "--matches", help="How many matches to return for each row. Default is 1.", type=int, default=1)
    parser.add_argument("-np", "--no_parallel", help="Flag that determines whether to use parallel processing to speed up search.", action="store_true")
    parser.add_argument("-a", "--add_ids", help="Flag that determines whether to add an id column to the data read from the input csv.", action="store_true")
    parser.add_argument("-s", "--skip_preprocessing", help="If set, skip preprocessing steps. This will slow down the processing.", action="store_true")

    args = parser.parse_args()

    stock_master = file_utils.read_csv(args.filename, add_ids=args.add_ids)
    level = args.level
    top_categories_to_check_count = args.num_to_check
    output = args.output
    jac = args.jaccard
    topn = args.matches
    parallel = not args.no_parallel
    skip_preprocessing = args.skip_preprocessing

    stime = time.time()

    if not output:
        stock_master = pandas.DataFrame(stock_master)
        df = add_commodities_to_dataframe(stock_master)
        print(df)
    else:
        rows = add_commodities_to_stocks(stock_master, level+" Name", top_categories_to_check_count, jac, topn, parallel, skip_preprocessing)
        try:
            file_utils.save_csv(output, rows, fieldnames=FIELDNAMES)
        except ValueError:
            print("Warning: row dictionaries contain keys not in fieldnames. Ignoring fieldnames...")
            file_utils.save_csv(output, rows)

    etime = time.time()
    ttime = etime-stime
    print('Time = ', ttime, 's')
